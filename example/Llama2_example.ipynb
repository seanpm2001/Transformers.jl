{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f37ead7f",
   "metadata": {},
   "source": [
    "This is a tutorial of how to use Large Language Model (LLM) with [Transformers.jl](https://github.com/chengchingwen/Transformers.jl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd8335e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Transformers, CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a5f1f1",
   "metadata": {},
   "source": [
    "After loading the package, we need to setup the gpu. Currently multi-gpu is not supported. If your machine have multiple gpu devices, we can use `CUDA.devices()` to get the list of all device and use `CUDA.device!(device_number)` to specify the device we want to run our model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "375362d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.DeviceIterator() for 8 devices:\n",
       "0. NVIDIA A100 80GB PCIe\n",
       "1. NVIDIA A100 80GB PCIe\n",
       "2. NVIDIA A100-PCIE-40GB\n",
       "3. Tesla V100-PCIE-32GB\n",
       "4. Tesla V100-PCIE-32GB\n",
       "5. Tesla V100S-PCIE-32GB\n",
       "6. Tesla V100-PCIE-32GB\n",
       "7. Tesla V100-PCIE-32GB"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f1009e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuDevice(1): NVIDIA A100 80GB PCIe"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.device!(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b1d392",
   "metadata": {},
   "source": [
    "For demonstration, we disable the scalar indexing on gpu so that we can make sure all gpu calls are handled without performance issue. By setting `enable_gpu`, we get a `todevice` provided by Transformers.jl that will move data/model to gpu device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36df972b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "todevice (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.allowscalar(false)\n",
    "enable_gpu(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc7fd2e",
   "metadata": {},
   "source": [
    "In this tutorial, we show how to do use the [llama-2-7b-chat (https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)](https://huggingface.co/databricks/dolly-v2-12b) in Julia.\n",
    "\n",
    "The process should also work for other causal LM based model. With Transformers.jl, we can get the tokenizer and model by using the `hgf\"\"` macro or `HuggingFace.load_tokenizer`/`HuggingFace.load_model`. The required files like the model weights will be downloaded and managed automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db63d2b",
   "metadata": {},
   "source": [
    "You would need a huggingface account that has access to llama2. Once you have the account, you need to copy your access token and pass it to Transformers.jl:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b024d5",
   "metadata": {},
   "source": [
    "```julia\n",
    "access_token = \"\"\n",
    "\n",
    "# This will save the access token to the disk, then all call to \n",
    "# download file from huggingface hub will use this token.\n",
    "using HuggingFaceApi\n",
    "HuggingFaceApi.save_token(access_token)\n",
    "\n",
    "# or call those `load` function with `auth_token` keyword argument\n",
    "# like this:\n",
    "HuggingFace.load_tokenizer(\"meta-llama/Llama-2-7b-chat-hf\"; auth_token = access_token)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b404517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mpadsym is set to `nothing`, using \"<pad>\" instead\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Transformers.HuggingFace ~/Transformers.jl/src/huggingface/tokenizer/utils.jl:96\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mpadsym <pad> not in vocabulary, this might cause problem.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Transformers.TextEncoders ~/Transformers.jl/src/textencoders/TextEncoders.jl:76\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HGFLlamaForCausalLM(\n",
       "  HGFLlamaModel(\n",
       "    CompositeEmbedding(\n",
       "      token = Embed(4096, 32000),       \u001b[90m# 131_072_000 parameters\u001b[39m\n",
       "    ),\n",
       "    Chain(\n",
       "      Transformer<32>(\n",
       "        PreNormTransformerBlock(\n",
       "          SelfAttention(\n",
       "            CausalGPTNeoXRoPEMultiheadQKVAttenOp(base = 10000.0, dim = 128, head = 32, p = nothing),\n",
       "            Fork<3>(Dense(W = (4096, 4096), b = false)),  \u001b[90m# 50_331_648 parameters\u001b[39m\n",
       "            Dense(W = (4096, 4096), b = false),  \u001b[90m# 16_777_216 parameters\u001b[39m\n",
       "          ),\n",
       "          RMSLayerNorm(4096, ϵ = 1.0e-6),  \u001b[90m# 4_096 parameters\u001b[39m\n",
       "          Chain(\n",
       "            LLamaGated(Dense(σ = NNlib.swish, W = (4096, 11008), b = false), Dense(W = (4096, 11008), b = false)),  \u001b[90m# 90_177_536 parameters\u001b[39m\n",
       "            Dense(W = (11008, 4096), b = false),  \u001b[90m# 45_088_768 parameters\u001b[39m\n",
       "          ),\n",
       "          RMSLayerNorm(4096, ϵ = 1.0e-6),  \u001b[90m# 4_096 parameters\u001b[39m\n",
       "        ),\n",
       "      ),\u001b[90m                  # Total: 288 arrays, \u001b[39m6_476_267_520 parameters, 55.266 KiB.\n",
       "      RMSLayerNorm(4096, ϵ = 1.0e-6),   \u001b[90m# 4_096 parameters\u001b[39m\n",
       "    ),\n",
       "  ),\n",
       "  Branch{(:logit,) = (:hidden_state,)}(\n",
       "    EmbedDecoder(Embed(4096, 32000)),   \u001b[90m# 131_072_000 parameters\u001b[39m\n",
       "  ),\n",
       ") \u001b[90m                  # Total: 291 arrays, \u001b[39m6_738_415_616 parameters, 69.766 KiB."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Transformers.HuggingFace\n",
    "\n",
    "textenc = hgf\"meta-llama/Llama-2-7b-chat-hf:tokenizer\"\n",
    "model = todevice(hgf\"meta-llama/Llama-2-7b-chat-hf:ForCausalLM\") # move to gpu with `todevice` (or `Flux.gpu`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c4be5b",
   "metadata": {},
   "source": [
    "We define some helper functions for the text generation. Here we are doing the simple greedy decoding. It can be replaced with other decoding algorithm like beam search. The `k` in `top_k_sample` decide the number of possible choices at each generation step. The default `k = 1` is simply `argmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc38fba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "top_k_sample (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux\n",
    "using StatsBase\n",
    "\n",
    "function temp_softmax(logits; temperature = 1.2)\n",
    "    return softmax(logits ./ temperature)\n",
    "end\n",
    "\n",
    "function top_k_sample(probs; k = 1)\n",
    "    sorted = sort(probs, rev = true)\n",
    "    indexes = partialsortperm(probs, 1:k, rev=true)\n",
    "    index = sample(indexes, ProbabilityWeights(sorted[1:k]), 1)\n",
    "    return index\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f6860a",
   "metadata": {},
   "source": [
    "The main generation loop is defined as follows:\n",
    "\n",
    "1. The prompt is first preprocessed and encoded with the tokenizer `textenc`. The `encode` function return a `NamedTuple` where `.token` is the one-hot representation of our context tokens.\n",
    "2. At each iteration, we copy the tokens to gpu and feed them to the model. The model also return a `NamedTuple` where `.logit` is the predictions of our model. We then apply the greedy decoding scheme to get the prediction of next token. The token will be appended to the end of context tokens. The iteration stop if we exceed the maximum generation length or the predicted token is an end token.\n",
    "3. After the loop, we decode the one-hot encoding back to text tokens. The `decode` function convert the onehots to texts and also perform some post-processing to get the final list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41169a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate_text (generic function with 2 methods)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Transformers.TextEncoders\n",
    "\n",
    "function generate_text(textenc, model, context = \"\"; max_length = 512, k = 1, temperature = 1.2, ends = textenc.endsym)\n",
    "    encoded = encode(textenc, context).token\n",
    "    ids = encoded.onehots\n",
    "    ends_id = lookup(textenc.vocab, ends)\n",
    "    for i in 1:max_length\n",
    "        input = (; token = encoded) |> todevice\n",
    "        outputs = model(input)\n",
    "        logits = @view outputs.logit[:, end, 1]\n",
    "        probs = temp_softmax(logits; temperature)\n",
    "        new_id = top_k_sample(collect(probs); k)[1]\n",
    "        push!(ids, new_id)\n",
    "        new_id == ends_id && break\n",
    "    end\n",
    "    return encoded\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f320111",
   "metadata": {},
   "source": [
    "We follow the prompt in [huggingface's llama2 blogpost](https://huggingface.co/blog/llama2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41a0c552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate(textenc, model, instruction; max_length = 512, k = 1, temperature = 1.2)\n",
    "    prompt = \"\"\"\n",
    "    [INST] <<SYS>>\n",
    "    You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "    If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "    <</SYS>>\n",
    "\n",
    "    $instruction [/INST]\n",
    "    \n",
    "    \"\"\"\n",
    "    text_token = generate_text(textenc, model, prompt; max_length, k, temperature)\n",
    "    gen_text = decode_text(textenc, text_token)\n",
    "    println(gen_text)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56f75981",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "Can you explain to me briefly what is the Julia programming language? [/INST]\n",
      "\n",
      "Of course! Julia is a high-level, high-performance programming language for technical computing. It was created in 2009 by Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral Shah. Julia is primarily designed for numerical and scientific computing, and it aims to provide a more efficient and expressive alternative to languages like Python, R, and Matlab.\n",
      "\n",
      "Some of the key features of Julia include:\n",
      "\n",
      "* High-level, high-performance: Julia is designed to be both easy to use and fast, with performance comparable to C++.\n",
      "* Dynamic typing: Julia is dynamically typed, which means that you don't need to declare variable types before using them. This can make code easier to write and read.\n",
      "* Multiple dispatch: Julia supports multiple dispatch, which means that functions can be defined with multiple methods that can be called depending on the types of the input arguments.\n",
      "* Macros: Julia has a powerful macro system that allows users to extend the language and create domain-specific languages (DSLs).\n",
      "* Interoperability: Julia has good interoperability with other languages, including Python, MATLAB, and C. This makes it easy to integrate code written in other languages into Julia programs.\n",
      "\n",
      "Overall, Julia is a powerful and flexible language that is well-suited for a wide range of scientific and numerical computing tasks. Its combination of high-level syntax and high-performance capabilities make it an attractive choice for researchers and engineers who need to perform complex computations quickly and efficiently.</s>\n"
     ]
    }
   ],
   "source": [
    "generate(textenc, model, \"Can you explain to me briefly what is the Julia programming language?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde6645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
